{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving a Lunar Lander with differentiable Genetic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To install the required libraries run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Imports from the standard genepro-multi library are done here. Any adjustments (e.g. different operators) should be made in the notebook. For example:\n",
    "\n",
    "```\n",
    "class SmoothOperator(Node):\n",
    "  def __init__(self):\n",
    "    super(SmoothOperator,self).__init__()\n",
    "    self.arity = 1\n",
    "    self.symb = \"SmoothOperator\"\n",
    "\n",
    "  def _get_args_repr(self, args):\n",
    "    return self._get_typical_repr(args,'before')\n",
    "\n",
    "  def get_output(self, X):\n",
    "    c_outs = self._get_child_outputs(X)\n",
    "    return np.smoothOperation(c_outs[0])\n",
    "\n",
    "  def get_output_pt(self, X):\n",
    "    c_outs = self._get_child_outputs_pt(X)\n",
    "    return torch.smoothOperation(c_outs[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from genepro.node_impl import *\n",
    "from genepro.evo import Evolution\n",
    "from genepro.node_impl import Constant\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Setup\n",
    "Here we first setup the Gymnasium environment. Please see https://gymnasium.farama.org/environments/box2d/lunar_lander/ for more information on the environment. \n",
    "\n",
    "Then a memory buffer is made. This is a buffer in which state transitions are stored. When the buffer reaches its maximum capacity old transitions are replaced by new ones.\n",
    "\n",
    "A frame buffer is initialised used to later store animation frames of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        self.memory += other.memory\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        self.memory = self.memory + other.memory\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Function\n",
    "\n",
    "Here you get to be creative. The default setup evaluates 5 episodes of 300 frames. Think of what action to pick and what fitness function to use. The Multi-tree takes an input of $n \\times d$ where $n$ is a batch of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def rollout_function(multitree, num_episodes=5, episode_duration=300, ignore_done=False, render=False, seed=None):\n",
    "    env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "\n",
    "    episode_rewards = np.zeros((num_episodes, episode_duration))\n",
    "    if render:\n",
    "        frames = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        # get initial state of the environment\n",
    "        # if a seed is given, set the seed of each episode\n",
    "        if seed:\n",
    "            observation = env.reset(seed=seed+i)\n",
    "        else:\n",
    "            observation = env.reset()\n",
    "        observation = observation[0]\n",
    "        rewards = np.zeros(episode_duration)\n",
    "        for j in range(episode_duration):\n",
    "            if render:\n",
    "                frames.append(env.render())\n",
    "            input_sample = observation.reshape((1, -1))\n",
    "            action = np.argmax(multitree.get_output(input_sample))\n",
    "            observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "            rewards[j] = reward\n",
    "            if (terminated or truncated) and not ignore_done:\n",
    "                break\n",
    "        episode_rewards[i, :] = rewards\n",
    "\n",
    "    if render:\n",
    "        return episode_rewards, [], frames\n",
    "    return episode_rewards, []\n",
    "\n",
    "def calculate_returns(episode_rewards, gamma=0.99):\n",
    "    num_steps = episode_rewards.shape[1]\n",
    "    g = np.power(gamma, np.arange(num_steps))\n",
    "    returns = np.sum(episode_rewards * g, axis=1)\n",
    "    return returns\n",
    "\n",
    "def fitness_baseline(episode_rewards, pop):\n",
    "    return np.mean(np.sum(episode_rewards, axis=1))\n",
    "\n",
    "def fitness_discounted_return(episode_rewards, pop):\n",
    "    return np.mean(calculate_returns(episode_rewards))\n",
    "\n",
    "def fitness_discounted_return_len_sqrt_std(episode_rewards, pop):\n",
    "    return np.mean(calculate_returns(episode_rewards)) - len(pop) - np.std(np.sum(episode_rewards, axis=1))**0.5\n",
    "\n",
    "def fitness_function_mean_len_sqrt_std(episode_rewards, pop):\n",
    "    return np.mean(np.sum(episode_rewards, axis=1)) - len(pop) - np.std(np.sum(episode_rewards, axis=1))**0.5\n",
    "\n",
    "def fitness_function_mean_len_std(episode_rewards, pop):\n",
    "    return np.mean(np.sum(episode_rewards, axis=1)) - len(pop) - np.std(np.sum(episode_rewards, axis=1))\n",
    "\n",
    "def fitness_function_mean_len_quarter_std(episode_rewards, pop):\n",
    "    return np.mean(np.sum(episode_rewards, axis=1)) - len(pop) - 0.25*np.std(np.sum(episode_rewards, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness function: symmetric approach\n",
    "\n",
    "For the side-correction thruster symmetry we will be using the same tree for the side thrusters. The only difference between the right and left thruster trees will be that some feature nodes that correspond with symmetry are inverted as follows:\n",
    "\n",
    "f(2)  \n",
    "$\\to$  \n",
    "```\n",
    "   *\n",
    "  / \\\n",
    "-1   f(2) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from genepro.thruster_symmetry import traverse_and_invert_iter\n",
    "\n",
    "\n",
    "def rollout_function_symmetry(multitree, num_episodes=5, episode_duration=300, ignore_done=False, render=False, seed=None):\n",
    "    env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "    \n",
    "    # here, we generate the right thruster tree using the left thruster tree\n",
    "    left_thruster_tree = deepcopy(multitree.children[1])\n",
    "    right_thruster_tree = left_thruster_tree\n",
    "    right_thruster_tree = traverse_and_invert_iter(left_thruster_tree, feature_ids = [0,2,4,5,6,7])\n",
    "\n",
    "    episode_rewards = []\n",
    "    if render:\n",
    "        frames = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        # get initial state of the environment\n",
    "        # if a seed is given, set the seed of each episode\n",
    "        if seed:\n",
    "            observation = env.reset(seed=seed+i)\n",
    "        else:\n",
    "            observation = env.reset()\n",
    "        observation = env.reset()\n",
    "        observation = observation[0]\n",
    "        rewards = []\n",
    "        for _ in range(episode_duration):\n",
    "            if render:\n",
    "                frames.append(env.render())\n",
    "            input_sample = observation.reshape((1, -1))\n",
    "            \n",
    "            outputs = multitree.get_output(input_sample)\n",
    "            outputs = np.append(outputs, right_thruster_tree.get_output(input_sample))\n",
    "            \n",
    "            action = np.argmax(outputs)\n",
    "            observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            if (terminated or truncated) and not ignore_done:\n",
    "                break\n",
    "        episode_rewards.append(np.sum(rewards))\n",
    "\n",
    "    # Get the average reward over all episodes\n",
    "    episode_rewards = np.array(episode_rewards)\n",
    "    if render:\n",
    "        return episode_rewards, [], frames\n",
    "    return episode_rewards, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USED TO STORE THE EXPERIMENT DICTIONARY\n",
    "import inspect\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "def serialize_functions_in_dict(dictionary):\n",
    "    for key, value in dictionary.items():\n",
    "        if inspect.isfunction(value) or inspect.ismethod(value):\n",
    "            dictionary[key] = value.__name__\n",
    "        elif isinstance(value, list):\n",
    "            for i, item in enumerate(value):\n",
    "                if isinstance(item, dict):\n",
    "                    value[i] = serialize_functions_in_dict(item)\n",
    "                elif inspect.isfunction(item) or inspect.ismethod(item):\n",
    "                    value[i] = item.__name__\n",
    "                elif isinstance(item, Node):\n",
    "                    value[i] = item.symb\n",
    "        elif isinstance(value, dict):\n",
    "            dictionary[key] = serialize_functions_in_dict(value)\n",
    "    return dictionary\n",
    "\n",
    "### USED TO CREATE THE EXPERIMENT DICTIONARY\n",
    "def grid_search_params(params_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary of hyperparameters, if a value is a list, loop over all values\n",
    "    and create a grid search.\n",
    "    \"\"\"\n",
    "    param_keys = params_dict.keys()\n",
    "    param_values = params_dict.values()\n",
    "    param_combinations = list(itertools.product(*[v if isinstance(v, list) else [v] for v in param_values]))\n",
    "    for combination in param_combinations:\n",
    "        yield dict(zip(param_keys, combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the gen as a pickle file in the gens folder\n",
    "def save_and_evaluate_evo_generations(evo, rollout_function, experiment_name, num_episodes=10):\n",
    "    generation_evo_fitnesses = []\n",
    "    generation_test_fitnesses = []\n",
    "    for i, gen in enumerate(evo.best_of_gens):\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        episode_rewards, _ = rollout_function(gen, num_episodes=num_episodes)\n",
    "        evo_fitness_mean, evo_fitness_std = round(np.mean(gen.fitnesses), 3), round(np.std(gen.fitnesses), 3)\n",
    "        test_fitness_mean, test_fitness_std  = round(np.mean(np.sum(episode_rewards, axis=1)), 3), round(np.std(np.sum(episode_rewards, axis=1)), 3)\n",
    "        print(f\"Best of Generation {i}: evo fitness:{evo_fitness_mean}+/-{evo_fitness_std} \\t test_fitness:{test_fitness_mean}+/-{test_fitness_std}\")\n",
    "        \n",
    "        generation_evo_fitnesses.append(gen.fitnesses)\n",
    "        generation_test_fitnesses.append(np.sum(episode_rewards, axis=1))\n",
    "        # create the gens folder if it doesn't exist\n",
    "        # os.makedirs(f\"./experiments/{experiment_name}/gen/\", exist_ok=True) \n",
    "        # with open(f\"./experiments/{experiment_name}/gen/gen_{i}_{evo_fitness_mean}_{test_fitness_mean}.pickle\", \"wb\") as f:\n",
    "        #     pickle.dump(gen, f)\n",
    "\n",
    "    np.save(f\"./experiments/{experiment_name}/generation_evo_fitnesses.npy\", generation_evo_fitnesses)\n",
    "    np.save(f\"./experiments/{experiment_name}/generation_test_fitnesses.npy\", generation_test_fitnesses)   \n",
    "    return generation_evo_fitnesses, generation_test_fitnesses\n",
    "\n",
    "def plot_evo_test_fitnesses(evo_fitnesses, test_fitnesses, experiment_name):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.set_title(f\"Fitnesses: {experiment_name}\")\n",
    "    ax.set_xlabel(\"Generation\")\n",
    "    ax.set_ylabel(\"Fitness\")\n",
    "    ax.plot(np.arange(len(evo_fitnesses)), [np.mean(gen) for gen in evo_fitnesses], label=\"evo_fitness\", color='tab:blue')\n",
    "    ax.fill_between(np.arange(len(evo_fitnesses)), [np.mean(gen) - np.std(gen) for gen in evo_fitnesses], [np.mean(gen) + np.std(gen) for gen in evo_fitnesses], alpha=0.2, color='tab:blue')\n",
    "    ax.plot(np.arange(len(test_fitnesses)), [np.mean(gen) for gen in test_fitnesses], label=\"test_fitness\", color='tab:orange')\n",
    "    ax.fill_between(np.arange(len(test_fitnesses)), [np.mean(gen) - np.std(gen) for gen in test_fitnesses], [np.mean(gen) + np.std(gen) for gen in test_fitnesses], alpha=0.2, color='tab:orange')\n",
    "    ax.legend()\n",
    "    plt.savefig(f\"./experiments/{experiment_name}/{experiment_name}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Setup\n",
    "Here the leaf and internal nodes are defined. Think about the odds of sampling a constant in this default configurations. Also think about any operators that could be useful and add them here. \n",
    "\n",
    "Adjust the population size (multiple of 8 if you want to use the standard tournament selection), max generations and max tree size to taste. Be aware that each of these settings can increase the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "## HERE ARE THE EXTRA SELECTION FUNCTIONS\n",
    "from genepro.selection import elitism_selection, tournament_selection, roulette_selection, boltzmann_selection, rank_selection\n",
    "from genepro.variation import coeff_mutation, subtree_crossover, subtree_mutation\n",
    "\n",
    "experiment_name = \"baseline\"\n",
    "num_features = env.observation_space.shape[0]\n",
    "evo_settings = {\n",
    "    \"rollout_function\": rollout_function,\n",
    "    \"fitness_function\": fitness_baseline,\n",
    "    \"internal_nodes\": [[Plus(), Times(), Div(), Sin(), Sqrt(), Square()]],\n",
    "    \"leaf_nodes\": [[Feature(i) for i in range(num_features)] + [Constant()]],\n",
    "    \"n_trees\": 4,\n",
    "    \"pop_size\": 32,\n",
    "    \"max_gens\": 500,\n",
    "    \"init_max_depth\": 4,\n",
    "    \"max_tree_size\": 32,\n",
    "    \"crossovers\": [[{\"fun\": subtree_crossover, \"rate\": 0.5}]],\n",
    "    \"mutations\": [[{\"fun\": subtree_mutation, \"rate\": 0.5}]],\n",
    "    \"coeff_opts\": [[{\"fun\": coeff_mutation, \"rate\": 0.5}]],\n",
    "    \"selection\": {\"fun\": tournament_selection, \"kwargs\": {\"tournament_size\": 4}},\n",
    "    \"n_jobs\": 8,\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FITNESS EXAMPLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_name = \"fitness\"\n",
    "num_features = env.observation_space.shape[0]\n",
    "evo_settings = {\n",
    "    \"rollout_function\": rollout_function,\n",
    "    \"fitness_function\": fitness_discounted_return,\n",
    "    \"internal_nodes\": [[Plus(), Times(), Div(), Sin(), Sqrt(), Square()]],\n",
    "    \"leaf_nodes\": [[Feature(i) for i in range(num_features)] + [Constant()]],\n",
    "    \"n_trees\": 4,\n",
    "    \"pop_size\": 32,\n",
    "    \"max_gens\": 500,\n",
    "    \"init_max_depth\": 4,\n",
    "    \"max_tree_size\": 32,\n",
    "    \"crossovers\": [[{\"fun\": subtree_crossover, \"rate\": 0.5}]],\n",
    "    \"mutations\": [[{\"fun\": subtree_mutation, \"rate\": 0.5}]],\n",
    "    \"coeff_opts\": [[{\"fun\": coeff_mutation, \"rate\": 0.5}]],\n",
    "    \"selection\": {\"fun\": tournament_selection, \"kwargs\": {\"tournament_size\": 4}},\n",
    "    \"n_jobs\": 8,\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SYMMETRY SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_name = \"symmetry\"\n",
    "num_features = env.observation_space.shape[0]\n",
    "evo_settings = {\n",
    "    \"rollout_function\": rollout_function_symmetry,\n",
    "    \"fitness_function\": fitness_baseline,\n",
    "    \"internal_nodes\": [[Plus(), Times(), Div(), Sin(), Sqrt(), Square()]],\n",
    "    \"leaf_nodes\": [[Feature(i) for i in range(num_features)] + [Constant()]],\n",
    "    \"n_trees\": 3,\n",
    "    \"pop_size\": 32,\n",
    "    \"max_gens\": 500,\n",
    "    \"init_max_depth\": 4,\n",
    "    \"max_tree_size\": 32,\n",
    "    \"crossovers\": [[{\"fun\": subtree_crossover, \"rate\": 0.5}]],\n",
    "    \"mutations\": [[{\"fun\": subtree_mutation, \"rate\": 0.5}]],\n",
    "    \"coeff_opts\": [[{\"fun\": coeff_mutation, \"rate\": 0.5}]],\n",
    "    \"selection\": {\"fun\": tournament_selection, \"kwargs\": {\"tournament_size\": 4}},\n",
    "    \"n_jobs\": 8,\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECTION SETTINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"selection\"\n",
    "num_features = env.observation_space.shape[0]\n",
    "evo_settings = {\n",
    "    \"rollout_function\": rollout_function,\n",
    "    \"fitness_function\": fitness_baseline,\n",
    "    \"internal_nodes\": [[Plus(), Times(), Div(), Sin(), Sqrt(), Square()]],\n",
    "    \"leaf_nodes\": [[Feature(i) for i in range(num_features)] + [Constant()]],\n",
    "    \"n_trees\": 3,\n",
    "    \"pop_size\": 32,\n",
    "    \"max_gens\": 500,\n",
    "    \"init_max_depth\": 4,\n",
    "    \"max_tree_size\": 32,\n",
    "    \"crossovers\": [[{\"fun\": subtree_crossover, \"rate\": 0.5}]],\n",
    "    \"mutations\": [[{\"fun\": subtree_mutation, \"rate\": 0.5}]],\n",
    "    \"coeff_opts\": [[{\"fun\": coeff_mutation, \"rate\": 0.5}]],\n",
    "    \"selection\": [[{\"fun\": tournament_selection, \"kwargs\": {\"tournament_size\": 4}}], \n",
    "                    [{\"fun\": elitism_selection}], \n",
    "                    [{\"fun\": roulette_selection}],\n",
    "                    [{\"fun\": boltzmann_selection, \"kwargs\": {\"temperature\": 0.5}}],\n",
    "                    [{\"fun\": boltzmann_selection, \"kwargs\": {\"temperature\": 10.0}}],\n",
    "                    [{\"fun\": rank_selection}]],\n",
    "    \"n_jobs\": 8,\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from genepro.selection import elitism_selection, tournament_selection, roulette_selection, boltzmann_selection, rank_selection\n",
    "from genepro.variation import coeff_mutation, subtree_crossover, subtree_mutation\n",
    "\n",
    "\n",
    "\n",
    "def hpo_evolve(evo_settings, experiment_name):\n",
    "    hpo_settings = list(grid_search_params(evo_settings))\n",
    "    for settings in hpo_settings:\n",
    "        serialized_dict = serialize_functions_in_dict(deepcopy(settings))\n",
    "        print(serialized_dict)\n",
    "        \n",
    "    for i, settings in enumerate(hpo_settings):\n",
    "        specific_experiment_name = experiment_name + f\"_pops{settings['pop_size']}_gens{settings['max_gens']}_mts{settings['max_tree_size']}_cor{settings['crossovers'][0]['rate']}_mutr{settings['mutations'][0]['rate']}_coeffr{settings['coeff_opts'][0]['rate']}\"\n",
    "        os.makedirs(f\"./experiments/{specific_experiment_name}\", exist_ok=True)\n",
    "        with open(f\"./experiments/{specific_experiment_name}/evo_settings.json\", \"w\") as f:\n",
    "            serialized_dict = serialize_functions_in_dict(deepcopy(settings))\n",
    "            json.dump(serialized_dict, f)\n",
    "\n",
    "        evo_baseline = Evolution(**settings)\n",
    "        evo_baseline.evolve()\n",
    "\n",
    "        with open(f\"./experiments/{specific_experiment_name}/evolution_class.pickle\", \"wb\") as f:\n",
    "            pickle.dump(evo_baseline, f)\n",
    "        \n",
    "        generation_evo_fitnesses, generation_test_fitnesses = save_and_evaluate_evo_generations(evo_baseline, evo_settings['rollout_function'], specific_experiment_name, num_episodes=5)\n",
    "        plot_evo_test_fitnesses(generation_evo_fitnesses, generation_test_fitnesses, specific_experiment_name)\n",
    "\n",
    "for i in range(n_experiments:=4):\n",
    "    hpo_evolve(evo_settings, experiment_name=f'{experiment_name}_exp{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code will reevaluate experiments by loading the evolution_class.pickle and retesting each best-gen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "def get_evolution_pickle_files(folder_prefix, root_dir='./experiments'):\n",
    "    evo_files = []\n",
    "    evo_setting_files = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if os.path.basename(root).startswith(folder_prefix):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file.startswith('evolution_class.pickle'):\n",
    "                    evo_files.append(file_path)\n",
    "                if file.startswith('evo_settings.json'):\n",
    "                    evo_setting_files.append(file_path)\n",
    "                    \n",
    "    return evo_files, evo_setting_files\n",
    "\n",
    "def re_evaluate_experiment(folder_prefix, rollout_function, root_dir='./experiments'):\n",
    "    evo_files, evo_setting_files = get_evolution_pickle_files(folder_prefix, root_dir)\n",
    "    for i, (evo_file, evo_setting_file) in enumerate(zip(evo_files, evo_setting_files)):\n",
    "        with open(evo_file, 'rb') as file:\n",
    "            evo = pickle.load(file)\n",
    "        print('Loaded:', evo_file)\n",
    "        \n",
    "        with open(evo_setting_file, 'rb') as file:\n",
    "            settings = json.load(file)\n",
    "        specific_experiment_name = folder_prefix + f\"reev_exp{i}_pops{settings['pop_size']}_gens{settings['max_gens']}_mts{settings['max_tree_size']}_cor{settings['crossovers'][0]['rate']}_mutr{settings['mutations'][0]['rate']}_coeffr{settings['coeff_opts'][0]['rate']}\"\n",
    "        print(specific_experiment_name)\n",
    "\n",
    "        generation_evo_fitnesses, generation_test_fitnesses = save_and_evaluate_evo_generations(evo, rollout_function, specific_experiment_name, num_episodes=5)\n",
    "        plot_evo_test_fitnesses(generation_evo_fitnesses, generation_test_fitnesses, specific_experiment_name)\n",
    "\n",
    "# re_evaluate_experiment(\"fitness_decay\", rollout_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating the effect of hyperparameter optimization\n",
    "\n",
    "We will look at what benefits hyperparameter optimization can have on our code.  We will be considering individuals from the baseline evolution as well as the symmetrical evolution experiments.\n",
    "\n",
    "### Step 1: get best individuals\n",
    "Each experiment has 500 individuals that were the best of their generation.  From each experiment, we will re-evaluate the performance of these individuals in a gym environment with a set random seed, and select the $n$ best individuals from each experiment.\n",
    "\n",
    "### Step 2: perform hyperparameter optimization\n",
    "On this list of individuals, consisting of $m \\times n$ individuals, where m is the number of experiment runs, we will perform coefficitent optimization.  Currently we will use the Q-learning code that has been provided to us, but we may also define a second optimization method if we have time.\n",
    "\n",
    "### Step 3: get test results\n",
    "Finally, we will get test results on a third test run, where we also set the random seed.  The resulting fitnesses will be plotted in a boxplot, where we can see the differences between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_fitness(multitree, rollout_function, fitness_function, seed):\n",
    "    \"\"\"\n",
    "    runs an individual multitree through a rollout and fitness function on an environment with a set seed\n",
    "    \"\"\"\n",
    "    test_fitness = fitness_function(*rollout_function(multitree, seed=seed))\n",
    "    print(f\"test score: {test_fitness}\")\n",
    "    return test_fitness\n",
    "\n",
    "\n",
    "def best_test_individuals(multitrees, rollout_function, fitness_function = fitness_baseline, n = 15, seed = 42):\n",
    "    \"\"\"\n",
    "    This function takes a list of individuals (such as the best_of_gens individuals from an experiment) and\n",
    "    re-runs them in an environment with a seed set by the `seed` argument.  It selects the `n` individuals with\n",
    "    the highest test score and returns them\n",
    "    \"\"\"\n",
    "    return sorted(multitrees, key=lambda m: get_test_fitness(m, rollout_function, fitness_function, seed), reverse=True)[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "def optimize_multitree_coefficients(multitree):\n",
    "    \"\"\"\n",
    "    this function uses q-learning to optimize the coefficients of a multitree.  It returns an optimized version of the input multitree\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "    GAMMA = 0.99\n",
    "\n",
    "    # Create a deep copy of the input multitree\n",
    "    optimized_tree = copy.deepcopy(multitree)\n",
    "\n",
    "    constants = optimized_tree.get_subtrees_consts()\n",
    "\n",
    "    if len(constants) > 0:\n",
    "        optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "    for _ in range(500):\n",
    "        if len(constants) > 0 and len(evo.memory) > batch_size:\n",
    "            target_tree = copy.deepcopy(optimized_tree)\n",
    "\n",
    "            transitions = evo.memory.sample(batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            non_final_mask = torch.tensor(\n",
    "                tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool\n",
    "            )\n",
    "\n",
    "            non_final_next_states = torch.cat(\n",
    "                [s for s in batch.next_state if s is not None]\n",
    "            )\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "            state_action_values = optimized_tree.get_output_pt(state_batch).gather(1, action_batch)\n",
    "            next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "            with torch.no_grad():\n",
    "                next_state_values[non_final_mask] = (\n",
    "                    target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "                )\n",
    "\n",
    "            expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "            criterion = nn.SmoothL1Loss()\n",
    "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "            optimizer.step()\n",
    "\n",
    "    return optimized_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load the pickle files of the experiments we want\n",
    "baseline_folder_prefix = \"baseline_exp\"\n",
    "symmetry_folder_prefix = \"fitness_symmetry\"\n",
    "\n",
    "baseline_pickle_files, _ = get_evolution_pickle_files(baseline_folder_prefix)\n",
    "symmetry_pickle_files, _ = get_evolution_pickle_files(symmetry_folder_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# then, we select the best indifiduals from each experiment run\n",
    "n_selected = 15\n",
    "random_seed = 10\n",
    "\n",
    "fitness_function = fitness_baseline\n",
    "\n",
    "baseline_individuals = []\n",
    "symmetry_individuals = []\n",
    "\n",
    "for i, file in enumerate(baseline_pickle_files):\n",
    "    with open(file, \"rb\") as f:\n",
    "        evo = pickle.load(f)\n",
    "    print(f\"processing pickle file {i}\")\n",
    "    print(evo)\n",
    "    best_of_gens = evo.best_of_gens\n",
    "    best_tested = best_test_individuals(best_of_gens, rollout_function, n=n_selected, seed=random_seed)\n",
    "    baseline_individuals.append(best_tested)\n",
    "\n",
    "for i, file in enumerate(symmetry_pickle_files):\n",
    "    with open(i, \"rb\") as f:\n",
    "        evo = pickle.load(f)\n",
    "    print(f\"processing pickle file {i}\")\n",
    "    best_of_gens = evo.best_of_gens\n",
    "    best_tested = best_test_individuals(best_of_gens, rollout_function_symmetry, n=n_selected, seed=random_seed)\n",
    "    symmetry_individuals.append(best_tested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the individuals to pickle files\n",
    "with open('./coefficient_optimization/baseline_individuals.pickle', 'wb') as f:\n",
    "    pickle.dump(baseline_individuals, f)\n",
    "    \n",
    "with open('./coefficient_optimization/symmetry_individuals.pickle', 'wb') as f:\n",
    "    pickle.dump(symmetry_individuals, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "class MultiTreeOptimizer:\n",
    "    def __init__(self, multitree, reward_function, learning_rate=1e-3):\n",
    "        self.multitree = multitree\n",
    "        self.reward_function = reward_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.constants = multitree.get_subtrees_consts()\n",
    "        self.optimizer = optim.AdamW(self.constants, lr=self.learning_rate, amsgrad=True)\n",
    "        self.loss_function = nn.SmoothL1Loss()\n",
    "\n",
    "    def optimize_constants(self):\n",
    "        if len(self.constants) > 0:\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            output = self.multitree(X)\n",
    "\n",
    "            fitness = self.fitness_function(output, y)\n",
    "\n",
    "            loss = self.loss_function(fitness, torch.tensor([0.0]))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_value_(self.constants, 100)\n",
    "\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def optimize_constants(multitree, reward_function, num_iterations=10, learning_rate=1e-3):\n",
    "    # Create a tensor from the multitree\n",
    "    optimized_multitree = copy.deepcopy(multitree)\n",
    "    constants = optimized_multitree.get_subtrees_consts()\n",
    "\n",
    "    # Define the Adam optimizer\n",
    "    optimizer = optim.AdamW(constants, lr=learning_rate, amsgrad=True)\n",
    "    print(constants)\n",
    "\n",
    "    # Perform optimization\n",
    "    for _ in range(num_iterations):\n",
    "        target_tree = copy.deepcopy(optimized_multitree)\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "        # Evaluate the reward function using the multitree tensor\n",
    "        reward = reward_function(optimized_multitree, rollout_function=rollout_function, fitness_function=fitness_baseline, seed=12)\n",
    "        reward = torch.tensor([reward], requires_grad=True)\n",
    "        \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        loss = criterion(reward, torch.tensor([300], dtype=float, requires_grad=True))\n",
    "        \n",
    "        print(loss)\n",
    "\n",
    "        # Backpropagation and gradient descent\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "        optimizer.step()\n",
    "\n",
    "    return optimized_multitree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = baseline_individuals[0][0]\n",
    "print(indiv)\n",
    "\n",
    "optimize_constants(indiv, get_test_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can optimize the individuals using the q-learning function\n",
    "if baseline_individuals:\n",
    "    baseline_optimized_individuals = [optimize_multitree_coefficients(multitree) for multitree in baseline_individuals]\n",
    "    \n",
    "if symmetry_individuals: \n",
    "    symmetry_optimized_individuals = [optimize_multitree_coefficients(multitree) for multitree in symmetry_individuals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_experiment_files(root_dir, folder_prefix):\n",
    "    evo_files = []\n",
    "    test_files = []\n",
    "    hyperparam_files = []\n",
    "    picke_files = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        if os.path.basename(root).startswith(folder_prefix):\n",
    "            for file in files:\n",
    "                if file.endswith('.npy'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    if 'evo' in file:\n",
    "                        evo_files.append(file_path)\n",
    "                    elif 'test' in file:\n",
    "                        test_files.append(file_path)\n",
    "                elif file.endswith('.json'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    hyperparam_files.append(file_path)\n",
    "                elif file.endswith('.pickle'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    picke_files.append(file_path)\n",
    "    return evo_files, test_files, hyperparam_files, picke_files\n",
    "\n",
    "def get_test_score(tree):\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(10):\n",
    "      # get initial state\n",
    "      observation = env.reset(seed=i)\n",
    "      observation = observation[0]\n",
    "\n",
    "      for _ in range(500):    \n",
    "        # build up the input sample for GP\n",
    "        input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        # get output (squeezing because it is encapsulated in an array)\n",
    "        output = tree.get_output_pt(input_sample)\n",
    "        action =  torch.argmax(output)\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        rewards.append(reward)\n",
    "\n",
    "\n",
    "        output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        if (terminated or truncated):\n",
    "            break\n",
    "\n",
    "    fitness = np.sum(rewards)\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "\n",
    "def create_boxplot(evo_results, test_results, optim_evo_results, optim_test_results, experiment_name, runs, n_best_gens=15, output_dir=\"optimisation\"):\n",
    "    # Create a boxplot with test and evo fitness\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    boxplot = ax.boxplot([evo_results, test_results, optim_evo_results, optim_test_results], patch_artist=True)\n",
    "\n",
    "    for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "        plt.setp(boxplot[element], color=\"black\")\n",
    "\n",
    "    evo_box = boxplot[\"boxes\"][0]\n",
    "    evo_box.set_facecolor((1, 0, 0, 0.2))\n",
    "    evo_box.set_edgecolor(\"black\")\n",
    "\n",
    "    test_box = boxplot[\"boxes\"][1]\n",
    "    test_box.set_facecolor((0, 0, 1, 0.2))\n",
    "    test_box.set_edgecolor(\"black\")  \n",
    "\n",
    "    evo_box_optim = boxplot[\"boxes\"][2]\n",
    "    evo_box_optim.set_facecolor((1, 0, 0, 0.2))\n",
    "    evo_box_optim.set_edgecolor(\"black\")\n",
    "\n",
    "    test_box_optim = boxplot[\"boxes\"][3]\n",
    "    test_box_optim.set_facecolor((0, 0, 1, 0.2))\n",
    "    test_box_optim.set_edgecolor(\"black\") \n",
    "\n",
    "    ax.legend([evo_box, test_box], [\"evo\", \"test\"])\n",
    "    ax.set_title(f\"Optimisation with fitness of best {n_best_gens} generations for {runs+1} baseline experiments\")\n",
    "    ax.set_ylabel(\"Fitness\")\n",
    "\n",
    "    plt.xticks([1, 2, 3, 4], [\"Without optimisation\", \"Without optimisation\", \"With optimisation\", \"With optimisation\"], rotation=45)\n",
    "\n",
    "    # have the x ticks rotated 90 degrees\n",
    "\n",
    "\n",
    "    plt.savefig(f\"{plot_dir}/{experiment_name}_fitness_boxplot.png\")\n",
    "\n",
    "# Load picke file\n",
    "\n",
    "output_dir = \"optimisation\"\n",
    "input_dir = \"experiments\"\n",
    "experiment_name = \"baseline_exp\"\n",
    "\n",
    "plot_dir = 'optimisation'\n",
    "\n",
    "\n",
    "evo_files, test_files, hyperparam_files, pickle_files = find_experiment_files(input_dir, experiment_name)\n",
    "assert len(evo_files) == len(test_files), \"Number of evolution files and test files must be the same\"\n",
    "assert len(evo_files) > 0, \"No evolution files found\"\n",
    "assert len(pickle_files) > 0, \"No pickle file found\"\n",
    "\n",
    "n_best_gens = 15\n",
    "num_episodes = 5\n",
    "\n",
    "total_evo_baseline = []\n",
    "total_test_baseline = []\n",
    "\n",
    "total_evo_optim = []\n",
    "total_test_optim = []\n",
    "\n",
    "total_test_score_baseline = []\n",
    "total_test_score_optim = []\n",
    "\n",
    "for n, pickle_file in enumerate(pickle_files):\n",
    "    print(f\"Loading picke file {n+1}\")    \n",
    "\n",
    "    gen_results = []\n",
    "\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        evo = pickle.load(f)\n",
    "\n",
    "    assert n_best_gens <= len(evo.best_of_gens), \"Number of best generations must be less than or equal to number of generations\"\n",
    "\n",
    "    # sort based on highest fitness as last gen\n",
    "    evo.best_of_gens.sort(key=lambda x: x.fitness)\n",
    "\n",
    "    for i in range(n_best_gens):\n",
    "        gen = evo.best_of_gens[-i-1]\n",
    "\n",
    "        gen_results.append(gen)\n",
    "\n",
    "        episode_rewards, _ = fitness_function(gen, num_episodes=num_episodes)\n",
    "        evo_fitness_mean, evo_fitness_std = round(np.mean(gen.fitnesses), 3), round(np.std(gen.fitnesses), 3)\n",
    "        test_fitness_mean, test_fitness_std  = round(np.mean(episode_rewards), 3), round(np.std(episode_rewards), 3)\n",
    "\n",
    "        total_evo_baseline.append(evo_fitness_mean)\n",
    "        total_test_baseline.append(test_fitness_mean)\n",
    "\n",
    "        total_test_score_baseline.append(get_test_score(gen))\n",
    "\n",
    "        # print(f\"Best of Generation {len(evo.best_of_gens) - i - 1}: evo fitness:{evo_fitness_mean}+/-{evo_fitness_std} \\t test_fitness:{test_fitness_mean}+/-{test_fitness_std}\")\n",
    "\n",
    "    # Optimisation\n",
    "    batch_size = 128\n",
    "    GAMMA = 0.99\n",
    "\n",
    "    print(f\"Start optimisation for picke file {n+1}\")\n",
    "\n",
    "    for gen in gen_results:\n",
    "        constants = gen.get_subtrees_consts()\n",
    "\n",
    "        if len(constants) > 0:\n",
    "            optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "        for _ in range(500):\n",
    "            if len(constants) > 0 and len(evo.memory) > batch_size:\n",
    "                target_tree = copy.deepcopy(gen)\n",
    "\n",
    "                transitions = evo.memory.sample(batch_size)\n",
    "                batch = Transition(*zip(*transitions))\n",
    "\n",
    "                non_final_mask = torch.tensor(\n",
    "                    tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool\n",
    "                )\n",
    "\n",
    "                non_final_next_states = torch.cat(\n",
    "                    [s for s in batch.next_state if s is not None]\n",
    "                )\n",
    "                state_batch = torch.cat(batch.state)\n",
    "                action_batch = torch.cat(batch.action)\n",
    "                reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "                state_action_values = gen.get_output_pt(state_batch).gather(1, action_batch)\n",
    "                next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "                with torch.no_grad():\n",
    "                    next_state_values[non_final_mask] = (\n",
    "                        target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "                    )\n",
    "\n",
    "                expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "                criterion = nn.SmoothL1Loss()\n",
    "                loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "                optimizer.step()\n",
    "\n",
    "        episode_rewards, _ = fitness_function(gen, num_episodes=num_episodes)\n",
    "        evo_fitness_mean, evo_fitness_std = round(np.mean(gen.fitnesses), 3), round(np.std(gen.fitnesses), 3)\n",
    "        test_fitness_mean, test_fitness_std  = round(np.mean(episode_rewards), 3), round(np.std(episode_rewards), 3)\n",
    "\n",
    "        total_evo_optim.append(evo_fitness_mean)\n",
    "        total_test_optim.append(test_fitness_mean)\n",
    "        \n",
    "        # print(f\"Best of Generation {len(evo.best_of_gens) - i - 1}: evo fitness:{evo_fitness_mean}+/-{evo_fitness_std} \\t test_fitness:{test_fitness_mean}+/-{test_fitness_std}\")\n",
    "        # print(get_test_score(gen))\n",
    "\n",
    "        total_test_score_optim.append(get_test_score(gen))\n",
    "\n",
    "\n",
    "\n",
    "create_boxplot(total_evo_baseline, total_test_baseline, total_evo_optim, total_test_optim, experiment_name, runs=n, n_best_gens=n_best_gens, output_dir=output_dir)\n",
    "\n",
    "# create boxplot for total_test_score\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "boxplot = ax.boxplot([total_test_score_baseline ,total_test_score_optim], patch_artist=True)\n",
    "\n",
    "for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
    "    plt.setp(boxplot[element], color=\"black\")\n",
    "\n",
    "evo_box = boxplot[\"boxes\"][0]\n",
    "evo_box.set_facecolor((1, 0, 0, 0.2))\n",
    "evo_box.set_edgecolor(\"black\")\n",
    "\n",
    "test_box = boxplot[\"boxes\"][1]\n",
    "test_box.set_facecolor((0, 0, 1, 0.2))\n",
    "test_box.set_edgecolor(\"black\") \n",
    "\n",
    "ax.legend([evo_box, test_box], [\"baseline\", \"optimised\"])\n",
    "ax.set_title(f\"Optimisation with score of best {n_best_gens} generations\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "\n",
    "# have the x ticks rotated 90 degrees\n",
    "\n",
    "\n",
    "plt.savefig(f\"{plot_dir}/{experiment_name}_score_boxplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an animation\n",
    "Here the best evolved individual is selected and one episode is rendered. Make sure to save your lunar landers over time to track progress and make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gist to save gif from https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553\n",
    "# def save_frames_as_gif(frames, path=\"./\", filename=\"evolved_lander.gif\"):\n",
    "#     plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "#     patch = plt.imshow(frames[0])\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     def animate(i):\n",
    "#         patch.set_data(frames[i])\n",
    "\n",
    "#     anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "#     anim.save(path + filename, writer=\"imagemagick\", fps=60)\n",
    "\n",
    "\n",
    "# frames = []\n",
    "# avg_fitness, frames = get_test_score(evo.best_of_gens[-1], num_episodes=5, episode_duration=300, seed=5, render=True)\n",
    "# print(\"Average fitness of the render is: \", avg_fitness)\n",
    "# env.close()\n",
    "# save_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "The coefficients in the multi-tree aren't optimised. Here Q-learning (taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) is used to optimise the weights further. Incorporate coefficient optimisation in training your agent(s). Coefficient Optimisation can be expensive. Think about how often you want to optimise, when, which individuals etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "# GAMMA = 0.99\n",
    "\n",
    "# constants = best.get_subtrees_consts()\n",
    "\n",
    "# if len(constants) > 0:\n",
    "#     optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "# for _ in range(500):\n",
    "#     if len(constants) > 0 and len(evo.memory) > batch_size:\n",
    "#         target_tree = copy.deepcopy(best)\n",
    "\n",
    "#         transitions = evo.memory.sample(batch_size)\n",
    "#         batch = Transition(*zip(*transitions))\n",
    "\n",
    "#         non_final_mask = torch.tensor(\n",
    "#             tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool\n",
    "#         )\n",
    "\n",
    "#         non_final_next_states = torch.cat(\n",
    "#             [s for s in batch.next_state if s is not None]\n",
    "#         )\n",
    "#         state_batch = torch.cat(batch.state)\n",
    "#         action_batch = torch.cat(batch.action)\n",
    "#         reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "#         state_action_values = best.get_output_pt(state_batch).gather(1, action_batch)\n",
    "#         next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "#         with torch.no_grad():\n",
    "#             next_state_values[non_final_mask] = (\n",
    "#                 target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "#             )\n",
    "\n",
    "#         expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "#         criterion = nn.SmoothL1Loss()\n",
    "#         loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "#         # Optimize the model\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "#         optimizer.step()\n",
    "\n",
    "# print(best.get_readable_repr())\n",
    "# print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = []\n",
    "# fitness_function_pt(\n",
    "#     best, num_episodes=1, episode_duration=500, render=True, ignore_done=False\n",
    "# )\n",
    "# env.close()\n",
    "# save_frames_as_gif(frames, filename=\"evolved_lander_RL.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander_RL.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
