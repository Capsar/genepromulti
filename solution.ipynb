{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving a Lunar Lander with differentiable Genetic Programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To install the required libraries run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Imports from the standard genepro-multi library are done here. Any adjustments (e.g. different operators) should be made in the notebook. For example:\n",
    "\n",
    "```\n",
    "class SmoothOperator(Node):\n",
    "  def __init__(self):\n",
    "    super(SmoothOperator,self).__init__()\n",
    "    self.arity = 1\n",
    "    self.symb = \"SmoothOperator\"\n",
    "\n",
    "  def _get_args_repr(self, args):\n",
    "    return self._get_typical_repr(args,'before')\n",
    "\n",
    "  def get_output(self, X):\n",
    "    c_outs = self._get_child_outputs(X)\n",
    "    return np.smoothOperation(c_outs[0])\n",
    "\n",
    "  def get_output_pt(self, X):\n",
    "    c_outs = self._get_child_outputs_pt(X)\n",
    "    return torch.smoothOperation(c_outs[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from genepro.node_impl import *\n",
    "from genepro.evo import Evolution\n",
    "from genepro.node_impl import Constant\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Setup\n",
    "Here we first setup the Gymnasium environment. Please see https://gymnasium.farama.org/environments/box2d/lunar_lander/ for more information on the environment. \n",
    "\n",
    "Then a memory buffer is made. This is a buffer in which state transitions are stored. When the buffer reaches its maximum capacity old transitions are replaced by new ones.\n",
    "\n",
    "A frame buffer is initialised used to later store animation frames of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        self.memory += other.memory\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        self.memory = self.memory + other.memory\n",
    "        return self"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Function\n",
    "\n",
    "Here you get to be creative. The default setup evaluates 5 episodes of 300 frames. Think of what action to pick and what fitness function to use. The Multi-tree takes an input of $n \\times d$ where $n$ is a batch of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_function_pt(multitree, num_episodes=10, episode_duration=300, ignore_done=False, render=False):\n",
    "    memory = ReplayMemory(10000)\n",
    "    rewards = []\n",
    "    if render:\n",
    "        frames = []\n",
    "\n",
    "    # print(multitree.get_readable_repr())\n",
    "    for _ in range(num_episodes):\n",
    "        # get initial state of the environment\n",
    "        observation = env.reset()\n",
    "        observation = observation[0]\n",
    "\n",
    "        for _ in range(episode_duration):\n",
    "            if render:\n",
    "                frames.append(env.render())\n",
    "            input_sample = torch.from_numpy(observation.reshape((1, -1))).float()\n",
    "            action = torch.argmax(multitree.get_output_pt(input_sample)).detach()\n",
    "            observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            output_sample = torch.from_numpy(observation.reshape((1, -1))).float()\n",
    "            memory.push(input_sample, torch.tensor([[action.item()]]), output_sample, torch.tensor([reward]))\n",
    "            if (terminated or truncated) and not ignore_done:\n",
    "                break\n",
    "\n",
    "    # Get the average reward over all episodes\n",
    "    fitness = np.sum(rewards) / num_episodes\n",
    "    if render:\n",
    "        return fitness, memory, frames\n",
    "    return fitness, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USED TO STORE THE EXPERIMENT DICTIONARY\n",
    "import inspect\n",
    "def serialize_functions_in_dict(dictionary):\n",
    "    for key, value in dictionary.items():\n",
    "        if inspect.isfunction(value) or inspect.ismethod(value):\n",
    "            dictionary[key] = value.__name__\n",
    "        elif isinstance(value, list):\n",
    "            for i, item in enumerate(value):\n",
    "                if isinstance(item, dict):\n",
    "                    value[i] = serialize_functions_in_dict(item)\n",
    "                elif inspect.isfunction(item) or inspect.ismethod(item):\n",
    "                    value[i] = item.__name__\n",
    "                elif isinstance(item, Node):\n",
    "                    value[i] = item.symb\n",
    "        elif isinstance(value, dict):\n",
    "            dictionary[key] = serialize_functions_in_dict(value)\n",
    "    return dictionary\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Setup\n",
    "Here the leaf and internal nodes are defined. Think about the odds of sampling a constant in this default configurations. Also think about any operators that could be useful and add them here. \n",
    "\n",
    "Adjust the population size (multiple of 8 if you want to use the standard tournament selection), max generations and max tree size to taste. Be aware that each of these settings can increase the runtime."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjacobs/miniconda3/envs/evolutionary-algorithms/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/benjacobs/miniconda3/envs/evolutionary-algorithms/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/benjacobs/miniconda3/envs/evolutionary-algorithms/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/benjacobs/miniconda3/envs/evolutionary-algorithms/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/benjacobs/miniconda3/envs/evolutionary-algorithms/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/benjacobs/miniconda3/envs/evolutionary-algorithms/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/benjacobs/miniconda3/envs/evolutionary-algorithms/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/benjacobs/miniconda3/envs/evolutionary-algorithms/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen: 1,\tbest of gen fitness: -105.501,\tbest of gen size: 29\n",
      "gen: 2,\tbest of gen fitness: -102.679,\tbest of gen size: 25\n",
      "gen: 3,\tbest of gen fitness: -84.577,\tbest of gen size: 27\n",
      "gen: 4,\tbest of gen fitness: -94.994,\tbest of gen size: 25\n",
      "gen: 5,\tbest of gen fitness: -65.011,\tbest of gen size: 27\n",
      "gen: 6,\tbest of gen fitness: -81.679,\tbest of gen size: 27\n",
      "gen: 7,\tbest of gen fitness: -84.251,\tbest of gen size: 27\n",
      "gen: 8,\tbest of gen fitness: -74.663,\tbest of gen size: 25\n",
      "gen: 9,\tbest of gen fitness: -50.218,\tbest of gen size: 25\n",
      "gen: 10,\tbest of gen fitness: -84.980,\tbest of gen size: 27\n",
      "gen: 11,\tbest of gen fitness: -82.277,\tbest of gen size: 25\n",
      "gen: 12,\tbest of gen fitness: -94.532,\tbest of gen size: 27\n",
      "gen: 13,\tbest of gen fitness: -78.919,\tbest of gen size: 25\n",
      "gen: 14,\tbest of gen fitness: -85.913,\tbest of gen size: 25\n",
      "gen: 15,\tbest of gen fitness: -84.466,\tbest of gen size: 31\n",
      "gen: 16,\tbest of gen fitness: -51.207,\tbest of gen size: 25\n",
      "gen: 17,\tbest of gen fitness: -77.305,\tbest of gen size: 25\n",
      "gen: 18,\tbest of gen fitness: -68.477,\tbest of gen size: 23\n",
      "gen: 19,\tbest of gen fitness: -84.793,\tbest of gen size: 23\n",
      "gen: 20,\tbest of gen fitness: -60.564,\tbest of gen size: 25\n",
      "gen: 21,\tbest of gen fitness: -46.423,\tbest of gen size: 27\n",
      "gen: 22,\tbest of gen fitness: -86.431,\tbest of gen size: 25\n",
      "gen: 23,\tbest of gen fitness: -85.699,\tbest of gen size: 31\n",
      "gen: 24,\tbest of gen fitness: -80.290,\tbest of gen size: 31\n",
      "gen: 25,\tbest of gen fitness: -68.636,\tbest of gen size: 31\n",
      "gen: 26,\tbest of gen fitness: -81.769,\tbest of gen size: 29\n",
      "gen: 27,\tbest of gen fitness: -71.202,\tbest of gen size: 31\n",
      "gen: 28,\tbest of gen fitness: -86.052,\tbest of gen size: 31\n",
      "gen: 29,\tbest of gen fitness: -83.987,\tbest of gen size: 27\n",
      "gen: 30,\tbest of gen fitness: -85.218,\tbest of gen size: 29\n",
      "gen: 31,\tbest of gen fitness: -84.795,\tbest of gen size: 31\n",
      "gen: 32,\tbest of gen fitness: -84.002,\tbest of gen size: 31\n",
      "gen: 33,\tbest of gen fitness: -68.248,\tbest of gen size: 31\n",
      "gen: 34,\tbest of gen fitness: -84.336,\tbest of gen size: 31\n",
      "gen: 35,\tbest of gen fitness: -67.758,\tbest of gen size: 31\n",
      "gen: 36,\tbest of gen fitness: -79.096,\tbest of gen size: 29\n",
      "gen: 37,\tbest of gen fitness: -71.381,\tbest of gen size: 31\n",
      "gen: 38,\tbest of gen fitness: -82.403,\tbest of gen size: 31\n",
      "gen: 39,\tbest of gen fitness: -76.867,\tbest of gen size: 31\n",
      "gen: 40,\tbest of gen fitness: -64.732,\tbest of gen size: 27\n",
      "gen: 41,\tbest of gen fitness: -67.123,\tbest of gen size: 27\n",
      "gen: 42,\tbest of gen fitness: -51.803,\tbest of gen size: 27\n",
      "gen: 43,\tbest of gen fitness: -70.605,\tbest of gen size: 27\n",
      "gen: 44,\tbest of gen fitness: -61.410,\tbest of gen size: 31\n",
      "gen: 45,\tbest of gen fitness: -64.395,\tbest of gen size: 31\n",
      "gen: 46,\tbest of gen fitness: -59.321,\tbest of gen size: 31\n",
      "gen: 47,\tbest of gen fitness: -43.209,\tbest of gen size: 31\n",
      "gen: 48,\tbest of gen fitness: -46.847,\tbest of gen size: 31\n",
      "gen: 49,\tbest of gen fitness: -62.722,\tbest of gen size: 31\n",
      "gen: 50,\tbest of gen fitness: -61.158,\tbest of gen size: 31\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from genepro.selection import tournament_selection\n",
    "from genepro.variation import coeff_mutation, subtree_crossover, subtree_mutation\n",
    "\n",
    "experiment_name = \"baseline\"\n",
    "num_features = env.observation_space.shape[0]\n",
    "evo_settings = {\n",
    "    \"fitness_function\": fitness_function_pt,\n",
    "    \"internal_nodes\": [Plus(), Minus(), Times(), Div()],\n",
    "    \"leaf_nodes\": [Feature(i) for i in range(num_features)] + [Constant()],\n",
    "    \"n_trees\": 4,\n",
    "    \"pop_size\": 64,\n",
    "    \"max_gens\": 50,\n",
    "    \"init_max_depth\": 4,\n",
    "    \"max_tree_size\": 32,\n",
    "    \"crossovers\": [{\"fun\": subtree_crossover, \"rate\": 0.5}],\n",
    "    \"mutations\": [{\"fun\": subtree_mutation, \"rate\": 0.5}],\n",
    "    \"coeff_opts\": [{\"fun\": coeff_mutation, \"rate\": 0.5}],\n",
    "    \"selection\": {\"fun\": tournament_selection, \"kwargs\": {\"tournament_size\": 8}},\n",
    "    \"n_jobs\": 8,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "os.makedirs(f\"./experiments/{experiment_name}\", exist_ok=True)\n",
    "with open(f\"./experiments/{experiment_name}/evo_settings.json\", \"w\") as f:\n",
    "    serialized_dict = serialize_functions_in_dict(deepcopy(evo_settings))\n",
    "    json.dump(serialized_dict, f)\n",
    "\n",
    "evo_baseline = Evolution(**evo_settings)\n",
    "evo_baseline.evolve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE BASELINE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best of Generation 0 : fitness -104.915 test_fitness: -213.01\n",
      "Best of Generation 1 : fitness -105.501 test_fitness: -142.088\n",
      "Best of Generation 2 : fitness -102.679 test_fitness: -121.829\n",
      "Best of Generation 3 : fitness -84.577 test_fitness: -131.358\n",
      "Best of Generation 4 : fitness -94.994 test_fitness: -120.945\n",
      "Best of Generation 5 : fitness -65.011 test_fitness: -117.902\n",
      "Best of Generation 6 : fitness -81.679 test_fitness: -118.86\n",
      "Best of Generation 7 : fitness -84.251 test_fitness: -114.12\n",
      "Best of Generation 8 : fitness -74.663 test_fitness: -114.436\n",
      "Best of Generation 9 : fitness -50.218 test_fitness: -107.026\n",
      "Best of Generation 10 : fitness -84.98 test_fitness: -114.802\n",
      "Best of Generation 11 : fitness -82.277 test_fitness: -120.994\n",
      "Best of Generation 12 : fitness -94.532 test_fitness: -132.281\n",
      "Best of Generation 13 : fitness -78.919 test_fitness: -115.383\n",
      "Best of Generation 14 : fitness -85.913 test_fitness: -110.752\n",
      "Best of Generation 15 : fitness -84.466 test_fitness: -111.533\n",
      "Best of Generation 16 : fitness -51.207 test_fitness: -108.099\n",
      "Best of Generation 17 : fitness -77.305 test_fitness: -100.324\n",
      "Best of Generation 18 : fitness -68.477 test_fitness: -101.465\n",
      "Best of Generation 19 : fitness -84.793 test_fitness: -111.146\n",
      "Best of Generation 20 : fitness -60.564 test_fitness: -119.812\n",
      "Best of Generation 21 : fitness -46.423 test_fitness: -110.732\n",
      "Best of Generation 22 : fitness -86.431 test_fitness: -131.852\n",
      "Best of Generation 23 : fitness -85.699 test_fitness: -103.942\n",
      "Best of Generation 24 : fitness -80.29 test_fitness: -105.23\n",
      "Best of Generation 25 : fitness -68.636 test_fitness: -122.176\n",
      "Best of Generation 26 : fitness -81.769 test_fitness: -100.131\n",
      "Best of Generation 27 : fitness -71.202 test_fitness: -115.354\n",
      "Best of Generation 28 : fitness -86.052 test_fitness: -102.191\n",
      "Best of Generation 29 : fitness -83.987 test_fitness: -113.545\n",
      "Best of Generation 30 : fitness -85.218 test_fitness: -139.497\n",
      "Best of Generation 31 : fitness -84.795 test_fitness: -105.078\n",
      "Best of Generation 32 : fitness -84.002 test_fitness: -99.68\n",
      "Best of Generation 33 : fitness -68.248 test_fitness: -95.48\n",
      "Best of Generation 34 : fitness -84.336 test_fitness: -101.786\n",
      "Best of Generation 35 : fitness -67.758 test_fitness: -92.145\n",
      "Best of Generation 36 : fitness -79.096 test_fitness: -123.272\n",
      "Best of Generation 37 : fitness -71.381 test_fitness: -101.255\n",
      "Best of Generation 38 : fitness -82.403 test_fitness: -95.298\n",
      "Best of Generation 39 : fitness -76.867 test_fitness: -102.431\n",
      "Best of Generation 40 : fitness -64.732 test_fitness: -109.882\n",
      "Best of Generation 41 : fitness -67.123 test_fitness: -118.771\n",
      "Best of Generation 42 : fitness -51.803 test_fitness: -98.212\n",
      "Best of Generation 43 : fitness -70.605 test_fitness: -102.698\n",
      "Best of Generation 44 : fitness -61.41 test_fitness: -72.241\n",
      "Best of Generation 45 : fitness -64.395 test_fitness: -81.169\n",
      "Best of Generation 46 : fitness -59.321 test_fitness: -74.678\n",
      "Best of Generation 47 : fitness -43.209 test_fitness: -71.816\n",
      "Best of Generation 48 : fitness -46.847 test_fitness: -76.623\n",
      "Best of Generation 49 : fitness -62.722 test_fitness: -82.673\n",
      "Best of Generation 50 : fitness -61.158 test_fitness: -70.759\n"
     ]
    }
   ],
   "source": [
    "# Save the gen as a pickle file in the gens folder\n",
    "import pickle\n",
    "def save_and_evaluate_evo_generations(evo, fitness_function):\n",
    "    test_fitnesses = []\n",
    "    for i, gen in enumerate(evo.best_of_gens):\n",
    "        avg_fitness, _ = fitness_function(gen, num_episodes=50)\n",
    "        print(\"Best of Generation\", i, \": fitness\", round(gen.fitness, 3), \"test_fitness:\", round(avg_fitness, 3))\n",
    "        test_fitnesses.append(avg_fitness)\n",
    "        # create the gens folder if it doesn't exist\n",
    "        with open(f\"./experiments/{experiment_name}/gen_{i}_{round(avg_fitness)}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(gen, f)\n",
    "    \n",
    "    return test_fitnesses\n",
    "\n",
    "save_and_evaluate_evo_generations(evo_baseline, fitness_function_pt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run multiple experiments and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (106832294.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    for _ in range num_experiments:\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "NUM_EXPERIMENTS = 2\n",
    "SAVE_DIR = \"./plots/plot_data\"\n",
    "experiment_name = \"plot_test\"\n",
    "\n",
    "evo_settings = {\n",
    "    \"fitness_function\": fitness_function_pt,\n",
    "    \"internal_nodes\": [Plus(), Minus(), Times(), Div()],\n",
    "    \"leaf_nodes\": [Feature(i) for i in range(num_features)] + [Constant()],\n",
    "    \"n_trees\": 4,\n",
    "    \"pop_size\": 8,\n",
    "    \"max_gens\": 10,\n",
    "    \"init_max_depth\": 4,\n",
    "    \"max_tree_size\": 32,\n",
    "    \"crossovers\": [{\"fun\": subtree_crossover, \"rate\": 0.5}],\n",
    "    \"mutations\": [{\"fun\": subtree_mutation, \"rate\": 0.5}],\n",
    "    \"coeff_opts\": [{\"fun\": coeff_mutation, \"rate\": 0.5}],\n",
    "    \"selection\": {\"fun\": tournament_selection, \"kwargs\": {\"tournament_size\": 8}},\n",
    "    \"n_jobs\": 8,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "os.makedirs(f\"./experiments/{experiment_name}\", exist_ok=True)\n",
    "with open(f\"./experiments/{experiment_name}/evo_settings.json\", \"w\") as f:\n",
    "    serialized_dict = serialize_functions_in_dict(deepcopy(evo_settings))\n",
    "    json.dump(serialized_dict, f)\n",
    "\n",
    "def run_multiple_evolutions(num_experiments, save_path):\n",
    "    fitnesses = []\n",
    "    \n",
    "    for _ in range(num_experiments):\n",
    "        evo_baseline = Evolution(**evo_settings)\n",
    "        evo_baseline.evolve()\n",
    "        fitness = save_and_evaluate_evo_generations(evo_baseline, fitness_function_pt)\n",
    "        fitnesses.append(fitness)\n",
    "    \n",
    "    np.save(save_path, fitnesses)\n",
    "\n",
    "save_path = f\"{SAVE_DIR}/{experiment_name}.npy\"\n",
    "\n",
    "run_multiple_evolutions(NUM_EXPERIMENTS, save_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolve\n",
    "Running this cell will use all the settings above as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an animation\n",
    "Here the best evolved individual is selected and one episode is rendered. Make sure to save your lunar landers over time to track progress and make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gist to save gif from https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553\n",
    "def save_frames_as_gif(frames, path=\"./\", filename=\"evolved_lander.gif\"):\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "    anim.save(path + filename, writer=\"imagemagick\", fps=60)\n",
    "\n",
    "\n",
    "frames = []\n",
    "avg_fitness, frames = get_test_score(evo.best_of_gens[-1], num_episodes=5, episode_duration=300, seed=5, render=True)\n",
    "print(\"Average fitness of the render is: \", avg_fitness)\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play animation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander.gif\" width=\"750\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "The coefficients in the multi-tree aren't optimised. Here Q-learning (taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) is used to optimise the weights further. Incorporate coefficient optimisation in training your agent(s). Coefficient Optimisation can be expensive. Think about how often you want to optimise, when, which individuals etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "constants = best.get_subtrees_consts()\n",
    "\n",
    "if len(constants) > 0:\n",
    "    optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "for _ in range(500):\n",
    "    if len(constants) > 0 and len(evo.memory) > batch_size:\n",
    "        target_tree = copy.deepcopy(best)\n",
    "\n",
    "        transitions = evo.memory.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool\n",
    "        )\n",
    "\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        )\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        state_action_values = best.get_output_pt(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = (\n",
    "                target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "            )\n",
    "\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "        optimizer.step()\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "fitness_function_pt(\n",
    "    best, num_episodes=1, episode_duration=500, render=True, ignore_done=False\n",
    ")\n",
    "env.close()\n",
    "save_frames_as_gif(frames, filename=\"evolved_lander_RL.gif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander_RL.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
